{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая нейронка для обработки абстрактного набора числовых данных"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 30 19:24:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "|  0%   59C    P0   104W / 350W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
    "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
    "xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale units\n",
    "X_max, _ = torch.max(X, 0)\n",
    "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
    "\n",
    "X = torch.div(X, X_max)\n",
    "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
    "y = y / 100  # max test score is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9200],\n",
      "        [1.0000],\n",
      "        [0.8900]]) cpu\n"
     ]
    }
   ],
   "source": [
    "# Tensor not on GPU\n",
    "print(y, y.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "X_on_gpu = X.to(device)\n",
    "xPredicted_on_gpu = xPredicted.to(device)\n",
    "y_on_gpu = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        # TODO: parameters can be parameterized instead of declaring them here\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "        \n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize).to(device) # 2 X 3 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize).to(device) # 3 X 1 tensor\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(X_test))\n",
    "        print (\"Output: \\n\" + str(self.forward(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.3143742084503174\n",
      "#1 Loss: 0.22764277458190918\n",
      "#2 Loss: 0.16210246086120605\n",
      "#3 Loss: 0.11674009263515472\n",
      "#4 Loss: 0.08628164231777191\n",
      "#5 Loss: 0.06572853773832321\n",
      "#6 Loss: 0.05156044661998749\n",
      "#7 Loss: 0.04152767360210419\n",
      "#8 Loss: 0.03422677516937256\n",
      "#9 Loss: 0.028776641935110092\n",
      "#10 Loss: 0.024613453075289726\n",
      "#11 Loss: 0.02136765420436859\n",
      "#12 Loss: 0.018791088834404945\n",
      "#13 Loss: 0.016712982207536697\n",
      "#14 Loss: 0.01501321792602539\n",
      "#15 Loss: 0.013605514541268349\n",
      "#16 Loss: 0.01242670975625515\n",
      "#17 Loss: 0.011429771780967712\n",
      "#18 Loss: 0.010579163208603859\n",
      "#19 Loss: 0.009847555309534073\n",
      "#20 Loss: 0.009213757701218128\n",
      "#21 Loss: 0.008661062456667423\n",
      "#22 Loss: 0.008176223374903202\n",
      "#23 Loss: 0.007748562376946211\n",
      "#24 Loss: 0.007369435857981443\n",
      "#25 Loss: 0.007031796034425497\n",
      "#26 Loss: 0.006729805842041969\n",
      "#27 Loss: 0.006458630785346031\n",
      "#28 Loss: 0.006214238237589598\n",
      "#29 Loss: 0.005993237718939781\n",
      "#30 Loss: 0.0057927402667701244\n",
      "#31 Loss: 0.005610307212918997\n",
      "#32 Loss: 0.005443856120109558\n",
      "#33 Loss: 0.005291580222547054\n",
      "#34 Loss: 0.005151932127773762\n",
      "#35 Loss: 0.005023573525249958\n",
      "#36 Loss: 0.004905317444354296\n",
      "#37 Loss: 0.004796165507286787\n",
      "#38 Loss: 0.004695208743214607\n",
      "#39 Loss: 0.004601658321917057\n",
      "#40 Loss: 0.004514817148447037\n",
      "#41 Loss: 0.0044340770691633224\n",
      "#42 Loss: 0.004358877427875996\n",
      "#43 Loss: 0.004288743250072002\n",
      "#44 Loss: 0.004223234951496124\n",
      "#45 Loss: 0.004161962307989597\n",
      "#46 Loss: 0.004104578867554665\n",
      "#47 Loss: 0.004050763323903084\n",
      "#48 Loss: 0.004000235814601183\n",
      "#49 Loss: 0.003952736966311932\n",
      "#50 Loss: 0.003908045589923859\n",
      "#51 Loss: 0.003865936305373907\n",
      "#52 Loss: 0.003826228901743889\n",
      "#53 Loss: 0.003788745030760765\n",
      "#54 Loss: 0.0037533349823206663\n",
      "#55 Loss: 0.0037198360078036785\n",
      "#56 Loss: 0.0036881249397993088\n",
      "#57 Loss: 0.0036580893211066723\n",
      "#58 Loss: 0.0036296010948717594\n",
      "#59 Loss: 0.0036025706212967634\n",
      "#60 Loss: 0.0035769015084952116\n",
      "#61 Loss: 0.0035524999257177114\n",
      "#62 Loss: 0.0035292943939566612\n",
      "#63 Loss: 0.003507201559841633\n",
      "#64 Loss: 0.0034861573949456215\n",
      "#65 Loss: 0.0034661057870835066\n",
      "#66 Loss: 0.0034469792153686285\n",
      "#67 Loss: 0.0034287250600755215\n",
      "#68 Loss: 0.003411291167140007\n",
      "#69 Loss: 0.0033946321345865726\n",
      "#70 Loss: 0.003378703026100993\n",
      "#71 Loss: 0.0033634649589657784\n",
      "#72 Loss: 0.003348874393850565\n",
      "#73 Loss: 0.003334903623908758\n",
      "#74 Loss: 0.003321513067930937\n",
      "#75 Loss: 0.00330867781303823\n",
      "#76 Loss: 0.0032963657286018133\n",
      "#77 Loss: 0.0032845463138073683\n",
      "#78 Loss: 0.00327319442294538\n",
      "#79 Loss: 0.003262285841628909\n",
      "#80 Loss: 0.0032518096268177032\n",
      "#81 Loss: 0.0032417315524071455\n",
      "#82 Loss: 0.0032320308964699507\n",
      "#83 Loss: 0.0032226992771029472\n",
      "#84 Loss: 0.0032137143425643444\n",
      "#85 Loss: 0.0032050542067736387\n",
      "#86 Loss: 0.0031967032700777054\n",
      "#87 Loss: 0.0031886501237750053\n",
      "#88 Loss: 0.0031808940693736076\n",
      "#89 Loss: 0.0031733966898173094\n",
      "#90 Loss: 0.0031661614775657654\n",
      "#91 Loss: 0.003159174695611\n",
      "#92 Loss: 0.003152427962049842\n",
      "#93 Loss: 0.003145900322124362\n",
      "#94 Loss: 0.0031395847909152508\n",
      "#95 Loss: 0.003133480902761221\n",
      "#96 Loss: 0.0031275737565010786\n",
      "#97 Loss: 0.0031218496151268482\n",
      "#98 Loss: 0.003116310341283679\n",
      "#99 Loss: 0.0031109412666410208\n",
      "#100 Loss: 0.0031057437881827354\n",
      "#101 Loss: 0.003100692993029952\n",
      "#102 Loss: 0.0030957995913922787\n",
      "#103 Loss: 0.0030910491477698088\n",
      "#104 Loss: 0.003086446551606059\n",
      "#105 Loss: 0.003081973409280181\n",
      "#106 Loss: 0.003077625297009945\n",
      "#107 Loss: 0.003073400352150202\n",
      "#108 Loss: 0.003069296246394515\n",
      "#109 Loss: 0.0030652969144284725\n",
      "#110 Loss: 0.0030614184215664864\n",
      "#111 Loss: 0.003057639580219984\n",
      "#112 Loss: 0.0030539645813405514\n",
      "#113 Loss: 0.0030503801535815\n",
      "#114 Loss: 0.0030468867626041174\n",
      "#115 Loss: 0.0030434890650212765\n",
      "#116 Loss: 0.003040173090994358\n",
      "#117 Loss: 0.003036944195628166\n",
      "#118 Loss: 0.003033796325325966\n",
      "#119 Loss: 0.003030722727999091\n",
      "#120 Loss: 0.003027718048542738\n",
      "#121 Loss: 0.0030247922986745834\n",
      "#122 Loss: 0.003021931741386652\n",
      "#123 Loss: 0.0030191331170499325\n",
      "#124 Loss: 0.003016402944922447\n",
      "#125 Loss: 0.0030137342400848866\n",
      "#126 Loss: 0.0030111237429082394\n",
      "#127 Loss: 0.0030085649341344833\n",
      "#128 Loss: 0.003006064798682928\n",
      "#129 Loss: 0.003003625199198723\n",
      "#130 Loss: 0.0030012293718755245\n",
      "#131 Loss: 0.0029988856986165047\n",
      "#132 Loss: 0.0029965857975184917\n",
      "#133 Loss: 0.002994334790855646\n",
      "#134 Loss: 0.0029921256937086582\n",
      "#135 Loss: 0.0029899566434323788\n",
      "#136 Loss: 0.002987832296639681\n",
      "#137 Loss: 0.0029857500921934843\n",
      "#138 Loss: 0.0029837049078196287\n",
      "#139 Loss: 0.0029816986061632633\n",
      "#140 Loss: 0.0029797262977808714\n",
      "#141 Loss: 0.002977789845317602\n",
      "#142 Loss: 0.0029758880846202374\n",
      "#143 Loss: 0.002974014263600111\n",
      "#144 Loss: 0.002972173038870096\n",
      "#145 Loss: 0.002970366505905986\n",
      "#146 Loss: 0.0029685883782804012\n",
      "#147 Loss: 0.002966836094856262\n",
      "#148 Loss: 0.002965109422802925\n",
      "#149 Loss: 0.0029634132515639067\n",
      "#150 Loss: 0.002961742226034403\n",
      "#151 Loss: 0.0029600977431982756\n",
      "#152 Loss: 0.002958474215120077\n",
      "#153 Loss: 0.0029568816535174847\n",
      "#154 Loss: 0.002955305390059948\n",
      "#155 Loss: 0.002953748684376478\n",
      "#156 Loss: 0.002952214330434799\n",
      "#157 Loss: 0.0029507025610655546\n",
      "#158 Loss: 0.002949213609099388\n",
      "#159 Loss: 0.002947734436020255\n",
      "#160 Loss: 0.0029462799429893494\n",
      "#161 Loss: 0.002944844774901867\n",
      "#162 Loss: 0.002943430095911026\n",
      "#163 Loss: 0.0029420210048556328\n",
      "#164 Loss: 0.002940637990832329\n",
      "#165 Loss: 0.0029392698779702187\n",
      "#166 Loss: 0.002937914803624153\n",
      "#167 Loss: 0.002936576260253787\n",
      "#168 Loss: 0.002935250988230109\n",
      "#169 Loss: 0.0029339399188756943\n",
      "#170 Loss: 0.0029326421208679676\n",
      "#171 Loss: 0.002931361785158515\n",
      "#172 Loss: 0.0029300893656909466\n",
      "#173 Loss: 0.0029288313817232847\n",
      "#174 Loss: 0.0029275817796587944\n",
      "#175 Loss: 0.002926345681771636\n",
      "#176 Loss: 0.002925124019384384\n",
      "#177 Loss: 0.002923913998529315\n",
      "#178 Loss: 0.002922712592408061\n",
      "#179 Loss: 0.0029215167742222548\n",
      "#180 Loss: 0.002920332830399275\n",
      "#181 Loss: 0.0029191607609391212\n",
      "#182 Loss: 0.0029180063866078854\n",
      "#183 Loss: 0.0029168501496315002\n",
      "#184 Loss: 0.002915704622864723\n",
      "#185 Loss: 0.0029145691078156233\n",
      "#186 Loss: 0.002913445234298706\n",
      "#187 Loss: 0.0029123257845640182\n",
      "#188 Loss: 0.0029112137854099274\n",
      "#189 Loss: 0.002910110168159008\n",
      "#190 Loss: 0.002909009112045169\n",
      "#191 Loss: 0.002907919231802225\n",
      "#192 Loss: 0.002906840993091464\n",
      "#193 Loss: 0.0029057671781629324\n",
      "#194 Loss: 0.0029047010466456413\n",
      "#195 Loss: 0.0029036374762654305\n",
      "#196 Loss: 0.0029025785624980927\n",
      "#197 Loss: 0.0029015287291258574\n",
      "#198 Loss: 0.0029004865791648626\n",
      "#199 Loss: 0.002899445593357086\n",
      "#200 Loss: 0.0028984134551137686\n",
      "#201 Loss: 0.0028973908629268408\n",
      "#202 Loss: 0.002896361518651247\n",
      "#203 Loss: 0.002895348472520709\n",
      "#204 Loss: 0.002894338220357895\n",
      "#205 Loss: 0.0028933268040418625\n",
      "#206 Loss: 0.002892326097935438\n",
      "#207 Loss: 0.0028913298156112432\n",
      "#208 Loss: 0.0028903321363031864\n",
      "#209 Loss: 0.0028893444687128067\n",
      "#210 Loss: 0.00288835889659822\n",
      "#211 Loss: 0.0028873751871287823\n",
      "#212 Loss: 0.0028864003252238035\n",
      "#213 Loss: 0.0028854277916252613\n",
      "#214 Loss: 0.002884453162550926\n",
      "#215 Loss: 0.002883484587073326\n",
      "#216 Loss: 0.0028825276531279087\n",
      "#217 Loss: 0.0028815686237066984\n",
      "#218 Loss: 0.002880606334656477\n",
      "#219 Loss: 0.002879653126001358\n",
      "#220 Loss: 0.0028787029441446066\n",
      "#221 Loss: 0.0028777567204087973\n",
      "#222 Loss: 0.0028768114279955626\n",
      "#223 Loss: 0.00287587265484035\n",
      "#224 Loss: 0.0028749345801770687\n",
      "#225 Loss: 0.002873996738344431\n",
      "#226 Loss: 0.0028730619233101606\n",
      "#227 Loss: 0.002872128738090396\n",
      "#228 Loss: 0.0028712020721286535\n",
      "#229 Loss: 0.002870272845029831\n",
      "#230 Loss: 0.00286935456097126\n",
      "#231 Loss: 0.0028684306889772415\n",
      "#232 Loss: 0.0028675091452896595\n",
      "#233 Loss: 0.0028665901627391577\n",
      "#234 Loss: 0.0028656781651079655\n",
      "#235 Loss: 0.0028647638391703367\n",
      "#236 Loss: 0.0028638557996600866\n",
      "#237 Loss: 0.002862943336367607\n",
      "#238 Loss: 0.0028620378579944372\n",
      "#239 Loss: 0.002861123299226165\n",
      "#240 Loss: 0.0028602275997400284\n",
      "#241 Loss: 0.0028593193273991346\n",
      "#242 Loss: 0.0028584212996065617\n",
      "#243 Loss: 0.0028575134929269552\n",
      "#244 Loss: 0.002856623847037554\n",
      "#245 Loss: 0.002855723723769188\n",
      "#246 Loss: 0.0028548359405249357\n",
      "#247 Loss: 0.0028539325576275587\n",
      "#248 Loss: 0.0028530415147542953\n",
      "#249 Loss: 0.0028521493077278137\n",
      "#250 Loss: 0.002851261757314205\n",
      "#251 Loss: 0.0028503749053925276\n",
      "#252 Loss: 0.002849483396857977\n",
      "#253 Loss: 0.0028485979419201612\n",
      "#254 Loss: 0.0028477106243371964\n",
      "#255 Loss: 0.0028468274977058172\n",
      "#256 Loss: 0.0028459466993808746\n",
      "#257 Loss: 0.002845069393515587\n",
      "#258 Loss: 0.0028441809117794037\n",
      "#259 Loss: 0.002843296155333519\n",
      "#260 Loss: 0.0028424214106053114\n",
      "#261 Loss: 0.0028415441047400236\n",
      "#262 Loss: 0.0028406642377376556\n",
      "#263 Loss: 0.002839786931872368\n",
      "#264 Loss: 0.0028389100916683674\n",
      "#265 Loss: 0.0028380383737385273\n",
      "#266 Loss: 0.002837158739566803\n",
      "#267 Loss: 0.0028362846933305264\n",
      "#268 Loss: 0.002835411112755537\n",
      "#269 Loss: 0.0028345424216240644\n",
      "#270 Loss: 0.0028336679097265005\n",
      "#271 Loss: 0.002832799218595028\n",
      "#272 Loss: 0.0028319284319877625\n",
      "#273 Loss: 0.0028310567140579224\n",
      "#274 Loss: 0.002830184530466795\n",
      "#275 Loss: 0.002829314675182104\n",
      "#276 Loss: 0.0028284480795264244\n",
      "#277 Loss: 0.002827579854056239\n",
      "#278 Loss: 0.0028267111629247665\n",
      "#279 Loss: 0.0028258459642529488\n",
      "#280 Loss: 0.002824980765581131\n",
      "#281 Loss: 0.0028241178952157497\n",
      "#282 Loss: 0.002823249204084277\n",
      "#283 Loss: 0.002822385635226965\n",
      "#284 Loss: 0.002821514382958412\n",
      "#285 Loss: 0.0028206538408994675\n",
      "#286 Loss: 0.0028197867795825005\n",
      "#287 Loss: 0.0028189278673380613\n",
      "#288 Loss: 0.0028180598746985197\n",
      "#289 Loss: 0.0028171981684863567\n",
      "#290 Loss: 0.002816336927935481\n",
      "#291 Loss: 0.002815467771142721\n",
      "#292 Loss: 0.0028146083932369947\n",
      "#293 Loss: 0.0028137434273958206\n",
      "#294 Loss: 0.002812885446473956\n",
      "#295 Loss: 0.002812022343277931\n",
      "#296 Loss: 0.0028111590072512627\n",
      "#297 Loss: 0.0028103000950068235\n",
      "#298 Loss: 0.0028094379231333733\n",
      "#299 Loss: 0.002808579010888934\n",
      "#300 Loss: 0.0028077156748622656\n",
      "#301 Loss: 0.002806860487908125\n",
      "#302 Loss: 0.002805996686220169\n",
      "#303 Loss: 0.0028051380068063736\n",
      "#304 Loss: 0.0028042742051184177\n",
      "#305 Loss: 0.002803415060043335\n",
      "#306 Loss: 0.0028025582432746887\n",
      "#307 Loss: 0.0028016967698931694\n",
      "#308 Loss: 0.002800833433866501\n",
      "#309 Loss: 0.002799977082759142\n",
      "#310 Loss: 0.0027991177048534155\n",
      "#311 Loss: 0.0027982580941170454\n",
      "#312 Loss: 0.0027973975520581007\n",
      "#313 Loss: 0.0027965372428297997\n",
      "#314 Loss: 0.0027956850826740265\n",
      "#315 Loss: 0.0027948215138167143\n",
      "#316 Loss: 0.002793960040435195\n",
      "#317 Loss: 0.0027931067161262035\n",
      "#318 Loss: 0.0027922415174543858\n",
      "#319 Loss: 0.002791382372379303\n",
      "#320 Loss: 0.002790527418255806\n",
      "#321 Loss: 0.0027896654792129993\n",
      "#322 Loss: 0.002788806101307273\n",
      "#323 Loss: 0.002787948353216052\n",
      "#324 Loss: 0.002787090837955475\n",
      "#325 Loss: 0.002786233788356185\n",
      "#326 Loss: 0.002785374876111746\n",
      "#327 Loss: 0.0027845173608511686\n",
      "#328 Loss: 0.0027836542576551437\n",
      "#329 Loss: 0.0027827906887978315\n",
      "#330 Loss: 0.0027819364331662655\n",
      "#331 Loss: 0.00278107519261539\n",
      "#332 Loss: 0.0027802176773548126\n",
      "#333 Loss: 0.002779357135295868\n",
      "#334 Loss: 0.0027785003185272217\n",
      "#335 Loss: 0.002777641173452139\n",
      "#336 Loss: 0.002776780165731907\n",
      "#337 Loss: 0.0027759214863181114\n",
      "#338 Loss: 0.002775062806904316\n",
      "#339 Loss: 0.00277419900521636\n",
      "#340 Loss: 0.0027733389288187027\n",
      "#341 Loss: 0.00277247978374362\n",
      "#342 Loss: 0.002771619241684675\n",
      "#343 Loss: 0.002770761027932167\n",
      "#344 Loss: 0.0027699049096554518\n",
      "#345 Loss: 0.0027690394781529903\n",
      "#346 Loss: 0.002768181962892413\n",
      "#347 Loss: 0.002767325146123767\n",
      "#348 Loss: 0.0027664608787745237\n",
      "#349 Loss: 0.002765595680102706\n",
      "#350 Loss: 0.002764738630503416\n",
      "#351 Loss: 0.0027638780884444714\n",
      "#352 Loss: 0.0027630177792161703\n",
      "#353 Loss: 0.0027621551416814327\n",
      "#354 Loss: 0.0027612950652837753\n",
      "#355 Loss: 0.0027604333590716124\n",
      "#356 Loss: 0.0027595716528594494\n",
      "#357 Loss: 0.0027587097138166428\n",
      "#358 Loss: 0.0027578454464673996\n",
      "#359 Loss: 0.0027569858357310295\n",
      "#360 Loss: 0.00275612436234951\n",
      "#361 Loss: 0.002755260793492198\n",
      "#362 Loss: 0.0027544009499251842\n",
      "#363 Loss: 0.0027535390108823776\n",
      "#364 Loss: 0.0027526782359927893\n",
      "#365 Loss: 0.0027518137358129025\n",
      "#366 Loss: 0.0027509499341249466\n",
      "#367 Loss: 0.002750086598098278\n",
      "#368 Loss: 0.002749219536781311\n",
      "#369 Loss: 0.002748363185673952\n",
      "#370 Loss: 0.0027474979870021343\n",
      "#371 Loss: 0.002746629063040018\n",
      "#372 Loss: 0.0027457657270133495\n",
      "#373 Loss: 0.0027449054177850485\n",
      "#374 Loss: 0.002744037192314863\n",
      "#375 Loss: 0.0027431733906269073\n",
      "#376 Loss: 0.002742314711213112\n",
      "#377 Loss: 0.0027414467185735703\n",
      "#378 Loss: 0.0027405822183936834\n",
      "#379 Loss: 0.002739713992923498\n",
      "#380 Loss: 0.0027388520538806915\n",
      "#381 Loss: 0.002737985923886299\n",
      "#382 Loss: 0.00273712445050478\n",
      "#383 Loss: 0.002736256457865238\n",
      "#384 Loss: 0.0027353917248547077\n",
      "#385 Loss: 0.0027345260605216026\n",
      "#386 Loss: 0.002733656670898199\n",
      "#387 Loss: 0.0027327891439199448\n",
      "#388 Loss: 0.002731921849772334\n",
      "#389 Loss: 0.002731058280915022\n",
      "#390 Loss: 0.002730186562985182\n",
      "#391 Loss: 0.0027293250896036625\n",
      "#392 Loss: 0.0027284591924399137\n",
      "#393 Loss: 0.0027275902684777975\n",
      "#394 Loss: 0.002726718783378601\n",
      "#395 Loss: 0.0027258514892309904\n",
      "#396 Loss: 0.002724985359236598\n",
      "#397 Loss: 0.0027241127099841833\n",
      "#398 Loss: 0.0027232468128204346\n",
      "#399 Loss: 0.0027223785873502493\n",
      "#400 Loss: 0.0027215080335736275\n",
      "#401 Loss: 0.0027206423692405224\n",
      "#402 Loss: 0.0027197699528187513\n",
      "#403 Loss: 0.002718900330364704\n",
      "#404 Loss: 0.0027180330362170935\n",
      "#405 Loss: 0.002717165043577552\n",
      "#406 Loss: 0.002716293092817068\n",
      "#407 Loss: 0.002715423470363021\n",
      "#408 Loss: 0.002714551752433181\n",
      "#409 Loss: 0.0027136802673339844\n",
      "#410 Loss: 0.0027128136716783047\n",
      "#411 Loss: 0.002711939625442028\n",
      "#412 Loss: 0.002711070701479912\n",
      "#413 Loss: 0.002710200846195221\n",
      "#414 Loss: 0.0027093254029750824\n",
      "#415 Loss: 0.0027084513567388058\n",
      "#416 Loss: 0.0027075796388089657\n",
      "#417 Loss: 0.002706710249185562\n",
      "#418 Loss: 0.0027058394625782967\n",
      "#419 Loss: 0.0027049630880355835\n",
      "#420 Loss: 0.0027040927670896053\n",
      "#421 Loss: 0.002703223144635558\n",
      "#422 Loss: 0.002702342811971903\n",
      "#423 Loss: 0.0027014734223484993\n",
      "#424 Loss: 0.0027006003074347973\n",
      "#425 Loss: 0.0026997216045856476\n",
      "#426 Loss: 0.0026988512836396694\n",
      "#427 Loss: 0.0026979739777743816\n",
      "#428 Loss: 0.0026970969047397375\n",
      "#429 Loss: 0.0026962263509631157\n",
      "#430 Loss: 0.00269535044208169\n",
      "#431 Loss: 0.002694475231692195\n",
      "#432 Loss: 0.0026936025824397802\n",
      "#433 Loss: 0.002692723646759987\n",
      "#434 Loss: 0.002691849134862423\n",
      "#435 Loss: 0.0026909734588116407\n",
      "#436 Loss: 0.002690098248422146\n",
      "#437 Loss: 0.002689218847081065\n",
      "#438 Loss: 0.002688345964998007\n",
      "#439 Loss: 0.0026874651666730642\n",
      "#440 Loss: 0.0026865871623158455\n",
      "#441 Loss: 0.0026857154443860054\n",
      "#442 Loss: 0.0026848353445529938\n",
      "#443 Loss: 0.0026839575730264187\n",
      "#444 Loss: 0.002683078870177269\n",
      "#445 Loss: 0.002682202262803912\n",
      "#446 Loss: 0.0026813182048499584\n",
      "#447 Loss: 0.0026804402004927397\n",
      "#448 Loss: 0.0026795631274580956\n",
      "#449 Loss: 0.0026786881498992443\n",
      "#450 Loss: 0.0026778073515743017\n",
      "#451 Loss: 0.002676926553249359\n",
      "#452 Loss: 0.002676046220585704\n",
      "#453 Loss: 0.002675171010196209\n",
      "#454 Loss: 0.002674288582056761\n",
      "#455 Loss: 0.002673408715054393\n",
      "#456 Loss: 0.0026725255884230137\n",
      "#457 Loss: 0.00267164409160614\n",
      "#458 Loss: 0.0026707667857408524\n",
      "#459 Loss: 0.002669884590432048\n",
      "#460 Loss: 0.0026690035592764616\n",
      "#461 Loss: 0.002668122760951519\n",
      "#462 Loss: 0.002667240798473358\n",
      "#463 Loss: 0.0026663579046726227\n",
      "#464 Loss: 0.002665475709363818\n",
      "#465 Loss: 0.0026645916514098644\n",
      "#466 Loss: 0.0026637110859155655\n",
      "#467 Loss: 0.002662828890606761\n",
      "#468 Loss: 0.0026619439013302326\n",
      "#469 Loss: 0.002661054953932762\n",
      "#470 Loss: 0.002660173922777176\n",
      "#471 Loss: 0.002659294754266739\n",
      "#472 Loss: 0.002658409997820854\n",
      "#473 Loss: 0.002657524775713682\n",
      "#474 Loss: 0.002656638389453292\n",
      "#475 Loss: 0.0026557534001767635\n",
      "#476 Loss: 0.002654870506376028\n",
      "#477 Loss: 0.0026539850514382124\n",
      "#478 Loss: 0.002653102157637477\n",
      "#479 Loss: 0.00265221344307065\n",
      "#480 Loss: 0.0026513307821005583\n",
      "#481 Loss: 0.0026504432316869497\n",
      "#482 Loss: 0.002649561269208789\n",
      "#483 Loss: 0.0026486702263355255\n",
      "#484 Loss: 0.0026477824430912733\n",
      "#485 Loss: 0.0026468958240002394\n",
      "#486 Loss: 0.0026460099034011364\n",
      "#487 Loss: 0.0026451232843101025\n",
      "#488 Loss: 0.002644231542944908\n",
      "#489 Loss: 0.0026433474849909544\n",
      "#490 Loss: 0.0026424587704241276\n",
      "#491 Loss: 0.0026415670290589333\n",
      "#492 Loss: 0.0026406836695969105\n",
      "#493 Loss: 0.0026397928595542908\n",
      "#494 Loss: 0.0026389039121568203\n",
      "#495 Loss: 0.002638016827404499\n",
      "#496 Loss: 0.0026371250860393047\n",
      "#497 Loss: 0.0026362359058111906\n",
      "#498 Loss: 0.002635346259921789\n",
      "#499 Loss: 0.002634456381201744\n",
      "#500 Loss: 0.002633567899465561\n",
      "#501 Loss: 0.002632677089422941\n",
      "#502 Loss: 0.0026317862793803215\n",
      "#503 Loss: 0.0026308968663215637\n",
      "#504 Loss: 0.002630004193633795\n",
      "#505 Loss: 0.0026291103567928076\n",
      "#506 Loss: 0.002628220245242119\n",
      "#507 Loss: 0.0026273312978446484\n",
      "#508 Loss: 0.0026264344342052937\n",
      "#509 Loss: 0.0026255431585013866\n",
      "#510 Loss: 0.0026246532797813416\n",
      "#511 Loss: 0.002623758977279067\n",
      "#512 Loss: 0.00262286770157516\n",
      "#513 Loss: 0.00262197433039546\n",
      "#514 Loss: 0.0026210814248770475\n",
      "#515 Loss: 0.0026201880536973476\n",
      "#516 Loss: 0.0026192953810095787\n",
      "#517 Loss: 0.002618396421894431\n",
      "#518 Loss: 0.0026175077073276043\n",
      "#519 Loss: 0.0026166103780269623\n",
      "#520 Loss: 0.0026157181710004807\n",
      "#521 Loss: 0.002614823170006275\n",
      "#522 Loss: 0.0026139277033507824\n",
      "#523 Loss: 0.0026130317710340023\n",
      "#524 Loss: 0.0026121377013623714\n",
      "#525 Loss: 0.0026112401392310858\n",
      "#526 Loss: 0.0026103458367288113\n",
      "#527 Loss: 0.002609449904412031\n",
      "#528 Loss: 0.0026085495483130217\n",
      "#529 Loss: 0.002607655245810747\n",
      "#530 Loss: 0.0026067569851875305\n",
      "#531 Loss: 0.0026058610528707504\n",
      "#532 Loss: 0.002604963257908821\n",
      "#533 Loss: 0.0026040691882371902\n",
      "#534 Loss: 0.002603173954412341\n",
      "#535 Loss: 0.0026022728998214006\n",
      "#536 Loss: 0.002601369982585311\n",
      "#537 Loss: 0.0026004731189459562\n",
      "#538 Loss: 0.002599574625492096\n",
      "#539 Loss: 0.002598677296191454\n",
      "#540 Loss: 0.0025977788027375937\n",
      "#541 Loss: 0.0025968763511627913\n",
      "#542 Loss: 0.0025959766935557127\n",
      "#543 Loss: 0.0025950775016099215\n",
      "#544 Loss: 0.002594180405139923\n",
      "#545 Loss: 0.002593278419226408\n",
      "#546 Loss: 0.002592377597466111\n",
      "#547 Loss: 0.0025914786383509636\n",
      "#548 Loss: 0.002590575022622943\n",
      "#549 Loss: 0.0025896751321852207\n",
      "#550 Loss: 0.0025887740775942802\n",
      "#551 Loss: 0.0025878760498017073\n",
      "#552 Loss: 0.0025869719684123993\n",
      "#553 Loss: 0.0025860697496682405\n",
      "#554 Loss: 0.0025851670652627945\n",
      "#555 Loss: 0.002584269968792796\n",
      "#556 Loss: 0.0025833663530647755\n",
      "#557 Loss: 0.002582462504506111\n",
      "#558 Loss: 0.0025815581902861595\n",
      "#559 Loss: 0.0025806566700339317\n",
      "#560 Loss: 0.002579753752797842\n",
      "#561 Loss: 0.0025788501370698214\n",
      "#562 Loss: 0.002577943727374077\n",
      "#563 Loss: 0.002577041042968631\n",
      "#564 Loss: 0.0025761350989341736\n",
      "#565 Loss: 0.002575230784714222\n",
      "#566 Loss: 0.0025743283331394196\n",
      "#567 Loss: 0.002573425183072686\n",
      "#568 Loss: 0.0025725197046995163\n",
      "#569 Loss: 0.002571613062173128\n",
      "#570 Loss: 0.00257070641964674\n",
      "#571 Loss: 0.002569799078628421\n",
      "#572 Loss: 0.0025688973255455494\n",
      "#573 Loss: 0.002567990217357874\n",
      "#574 Loss: 0.002567081479355693\n",
      "#575 Loss: 0.0025661762338131666\n",
      "#576 Loss: 0.002565270522609353\n",
      "#577 Loss: 0.002564361086115241\n",
      "#578 Loss: 0.002563455142080784\n",
      "#579 Loss: 0.002562546171247959\n",
      "#580 Loss: 0.0025616397615522146\n",
      "#581 Loss: 0.00256073335185647\n",
      "#582 Loss: 0.0025598257780075073\n",
      "#583 Loss: 0.002558918436989188\n",
      "#584 Loss: 0.002558005042374134\n",
      "#585 Loss: 0.002557097002863884\n",
      "#586 Loss: 0.0025561871007084846\n",
      "#587 Loss: 0.002555282786488533\n",
      "#588 Loss: 0.002554373349994421\n",
      "#589 Loss: 0.0025534569285809994\n",
      "#590 Loss: 0.002552550518885255\n",
      "#591 Loss: 0.0025516392197459936\n",
      "#592 Loss: 0.0025507293175905943\n",
      "#593 Loss: 0.0025498198810964823\n",
      "#594 Loss: 0.00254890788346529\n",
      "#595 Loss: 0.0025479947216808796\n",
      "#596 Loss: 0.0025470887776464224\n",
      "#597 Loss: 0.0025461760815232992\n",
      "#598 Loss: 0.0025452671106904745\n",
      "#599 Loss: 0.0025443527847528458\n",
      "#600 Loss: 0.00254344055429101\n",
      "#601 Loss: 0.0025425287894904613\n",
      "#602 Loss: 0.0025416119024157524\n",
      "#603 Loss: 0.0025407043285667896\n",
      "#604 Loss: 0.00253978930413723\n",
      "#605 Loss: 0.0025388780049979687\n",
      "#606 Loss: 0.00253796367906034\n",
      "#607 Loss: 0.0025370509829372168\n",
      "#608 Loss: 0.0025361310690641403\n",
      "#609 Loss: 0.0025352200027555227\n",
      "#610 Loss: 0.0025343073066323996\n",
      "#611 Loss: 0.002533392980694771\n",
      "#612 Loss: 0.002532477956265211\n",
      "#613 Loss: 0.0025315654929727316\n",
      "#614 Loss: 0.00253065163269639\n",
      "#615 Loss: 0.0025297319516539574\n",
      "#616 Loss: 0.0025288190227001905\n",
      "#617 Loss: 0.002527898643165827\n",
      "#618 Loss: 0.0025269859470427036\n",
      "#619 Loss: 0.002526072785258293\n",
      "#620 Loss: 0.0025251496117562056\n",
      "#621 Loss: 0.002524237846955657\n",
      "#622 Loss: 0.0025233207270503044\n",
      "#623 Loss: 0.0025224038399755955\n",
      "#624 Loss: 0.0025214895140379667\n",
      "#625 Loss: 0.002520571695640683\n",
      "#626 Loss: 0.0025196517817676067\n",
      "#627 Loss: 0.002518734661862254\n",
      "#628 Loss: 0.002517813118174672\n",
      "#629 Loss: 0.002516898326575756\n",
      "#630 Loss: 0.002515980042517185\n",
      "#631 Loss: 0.002515061292797327\n",
      "#632 Loss: 0.0025141448713839054\n",
      "#633 Loss: 0.0025132230948656797\n",
      "#634 Loss: 0.0025123064406216145\n",
      "#635 Loss: 0.002511382568627596\n",
      "#636 Loss: 0.0025104638189077377\n",
      "#637 Loss: 0.002509546699002385\n",
      "#638 Loss: 0.0025086216628551483\n",
      "#639 Loss: 0.0025077033787965775\n",
      "#640 Loss: 0.002506786957383156\n",
      "#641 Loss: 0.002505861921235919\n",
      "#642 Loss: 0.0025049448013305664\n",
      "#643 Loss: 0.0025040225591510534\n",
      "#644 Loss: 0.0025030996184796095\n",
      "#645 Loss: 0.0025021792389452457\n",
      "#646 Loss: 0.002501253504306078\n",
      "#647 Loss: 0.0025003314949572086\n",
      "#648 Loss: 0.0024994160048663616\n",
      "#649 Loss: 0.0024984912015497684\n",
      "#650 Loss: 0.002497566631063819\n",
      "#651 Loss: 0.002496646251529455\n",
      "#652 Loss: 0.0024957219138741493\n",
      "#653 Loss: 0.0024947975762188435\n",
      "#654 Loss: 0.002493880223482847\n",
      "#655 Loss: 0.0024929556529968977\n",
      "#656 Loss: 0.002492026425898075\n",
      "#657 Loss: 0.002491101622581482\n",
      "#658 Loss: 0.002490179380401969\n",
      "#659 Loss: 0.0024892583023756742\n",
      "#660 Loss: 0.002488329540938139\n",
      "#661 Loss: 0.0024874061346054077\n",
      "#662 Loss: 0.0024864845909178257\n",
      "#663 Loss: 0.002485555363819003\n",
      "#664 Loss: 0.0024846289306879044\n",
      "#665 Loss: 0.0024837057571858168\n",
      "#666 Loss: 0.0024827769957482815\n",
      "#667 Loss: 0.0024818475358188152\n",
      "#668 Loss: 0.0024809257593005896\n",
      "#669 Loss: 0.002479997929185629\n",
      "#670 Loss: 0.0024790740571916103\n",
      "#671 Loss: 0.0024781455285847187\n",
      "#672 Loss: 0.0024772146716713905\n",
      "#673 Loss: 0.002476288704201579\n",
      "#674 Loss: 0.002475363202393055\n",
      "#675 Loss: 0.0024744360707700253\n",
      "#676 Loss: 0.0024735108017921448\n",
      "#677 Loss: 0.002472582273185253\n",
      "#678 Loss: 0.0024716476909816265\n",
      "#679 Loss: 0.002470725914463401\n",
      "#680 Loss: 0.00246979808434844\n",
      "#681 Loss: 0.0024688642006367445\n",
      "#682 Loss: 0.002467936836183071\n",
      "#683 Loss: 0.0024670069105923176\n",
      "#684 Loss: 0.0024660793133080006\n",
      "#685 Loss: 0.0024651451967656612\n",
      "#686 Loss: 0.0024642162024974823\n",
      "#687 Loss: 0.002463290235027671\n",
      "#688 Loss: 0.002462356351315975\n",
      "#689 Loss: 0.002461428055539727\n",
      "#690 Loss: 0.0024604934733361006\n",
      "#691 Loss: 0.0024595651775598526\n",
      "#692 Loss: 0.0024586329236626625\n",
      "#693 Loss: 0.0024576992727816105\n",
      "#694 Loss: 0.0024567709770053625\n",
      "#695 Loss: 0.002455837558954954\n",
      "#696 Loss: 0.002454909263178706\n",
      "#697 Loss: 0.0024539707228541374\n",
      "#698 Loss: 0.0024530428927391768\n",
      "#699 Loss: 0.002452113898470998\n",
      "#700 Loss: 0.002451174659654498\n",
      "#701 Loss: 0.002450244966894388\n",
      "#702 Loss: 0.0024493089877068996\n",
      "#703 Loss: 0.0024483785964548588\n",
      "#704 Loss: 0.0024474470410495996\n",
      "#705 Loss: 0.002446511760354042\n",
      "#706 Loss: 0.0024455746170133352\n",
      "#707 Loss: 0.0024446453899145126\n",
      "#708 Loss: 0.0024437091778963804\n",
      "#709 Loss: 0.002442772965878248\n",
      "#710 Loss: 0.002441839314997196\n",
      "#711 Loss: 0.002440899610519409\n",
      "#712 Loss: 0.002439970150589943\n",
      "#713 Loss: 0.002439031843096018\n",
      "#714 Loss: 0.0024380998220294714\n",
      "#715 Loss: 0.0024371608160436153\n",
      "#716 Loss: 0.002436223439872265\n",
      "#717 Loss: 0.0024352900218218565\n",
      "#718 Loss: 0.00243435800075531\n",
      "#719 Loss: 0.0024334180634468794\n",
      "#720 Loss: 0.0024324790574610233\n",
      "#721 Loss: 0.0024315454065799713\n",
      "#722 Loss: 0.0024306064005941153\n",
      "#723 Loss: 0.0024296699557453394\n",
      "#724 Loss: 0.0024287353735417128\n",
      "#725 Loss: 0.0024277977645397186\n",
      "#726 Loss: 0.002426854334771633\n",
      "#727 Loss: 0.0024259164929389954\n",
      "#728 Loss: 0.0024249819107353687\n",
      "#729 Loss: 0.0024240438360720873\n",
      "#730 Loss: 0.0024231048300862312\n",
      "#731 Loss: 0.0024221634957939386\n",
      "#732 Loss: 0.00242122495546937\n",
      "#733 Loss: 0.002420288510620594\n",
      "#734 Loss: 0.0024193450808525085\n",
      "#735 Loss: 0.0024184081703424454\n",
      "#736 Loss: 0.0024174642749130726\n",
      "#737 Loss: 0.0024165273644030094\n",
      "#738 Loss: 0.0024155844002962112\n",
      "#739 Loss: 0.0024146409705281258\n",
      "#740 Loss: 0.0024137054570019245\n",
      "#741 Loss: 0.0024127669166773558\n",
      "#742 Loss: 0.002411826513707638\n",
      "#743 Loss: 0.002410882618278265\n",
      "#744 Loss: 0.002409942215308547\n",
      "#745 Loss: 0.0024090008810162544\n",
      "#746 Loss: 0.0024080572184175253\n",
      "#747 Loss: 0.0024071182124316692\n",
      "#748 Loss: 0.0024061778094619513\n",
      "#749 Loss: 0.002405235543847084\n",
      "#750 Loss: 0.0024042895529419184\n",
      "#751 Loss: 0.0024033482186496258\n",
      "#752 Loss: 0.002402407815679908\n",
      "#753 Loss: 0.002401466015726328\n",
      "#754 Loss: 0.0024005204904824495\n",
      "#755 Loss: 0.0023995735682547092\n",
      "#756 Loss: 0.002398633398115635\n",
      "#757 Loss: 0.002397689037024975\n",
      "#758 Loss: 0.0023967474699020386\n",
      "#759 Loss: 0.0023957989178597927\n",
      "#760 Loss: 0.0023948540911078453\n",
      "#761 Loss: 0.0023939141537994146\n",
      "#762 Loss: 0.002392972819507122\n",
      "#763 Loss: 0.0023920214734971523\n",
      "#764 Loss: 0.0023910775780677795\n",
      "#765 Loss: 0.002390132751315832\n",
      "#766 Loss: 0.00238918699324131\n",
      "#767 Loss: 0.0023882396053522825\n",
      "#768 Loss: 0.002387296175584197\n",
      "#769 Loss: 0.0023863473907113075\n",
      "#770 Loss: 0.0023854062892496586\n",
      "#771 Loss: 0.002384456805884838\n",
      "#772 Loss: 0.002383515704423189\n",
      "#773 Loss: 0.0023825650569051504\n",
      "#774 Loss: 0.002381617669016123\n",
      "#775 Loss: 0.0023806740064173937\n",
      "#776 Loss: 0.0023797235917299986\n",
      "#777 Loss: 0.00237877550534904\n",
      "#778 Loss: 0.0023778274189680815\n",
      "#779 Loss: 0.002376880496740341\n",
      "#780 Loss: 0.0023759333416819572\n",
      "#781 Loss: 0.002374984323978424\n",
      "#782 Loss: 0.002374033909291029\n",
      "#783 Loss: 0.0023730879183858633\n",
      "#784 Loss: 0.0023721407633274794\n",
      "#785 Loss: 0.0023711889516562223\n",
      "#786 Loss: 0.0023702438920736313\n",
      "#787 Loss: 0.0023692927788943052\n",
      "#788 Loss: 0.002368345856666565\n",
      "#789 Loss: 0.0023673968389630318\n",
      "#790 Loss: 0.0023664440959692\n",
      "#791 Loss: 0.0023654955439269543\n",
      "#792 Loss: 0.0023645455949008465\n",
      "#793 Loss: 0.0023635937832295895\n",
      "#794 Loss: 0.002362646395340562\n",
      "#795 Loss: 0.0023616922553628683\n",
      "#796 Loss: 0.0023607430048286915\n",
      "#797 Loss: 0.002359793521463871\n",
      "#798 Loss: 0.002358840312808752\n",
      "#799 Loss: 0.002357889898121357\n",
      "#800 Loss: 0.0023569404147565365\n",
      "#801 Loss: 0.002355987671762705\n",
      "#802 Loss: 0.0023550374899059534\n",
      "#803 Loss: 0.0023540835827589035\n",
      "#804 Loss: 0.002353131305426359\n",
      "#805 Loss: 0.0023521804250776768\n",
      "#806 Loss: 0.002351229079067707\n",
      "#807 Loss: 0.0023502742405980825\n",
      "#808 Loss: 0.0023493224289268255\n",
      "#809 Loss: 0.002348370850086212\n",
      "#810 Loss: 0.0023474181070923805\n",
      "#811 Loss: 0.002346463967114687\n",
      "#812 Loss: 0.0023455098271369934\n",
      "#813 Loss: 0.002344551496207714\n",
      "#814 Loss: 0.002343599684536457\n",
      "#815 Loss: 0.0023426469415426254\n",
      "#816 Loss: 0.0023416930343955755\n",
      "#817 Loss: 0.0023407426197081804\n",
      "#818 Loss: 0.0023397814948111773\n",
      "#819 Loss: 0.002338831080123782\n",
      "#820 Loss: 0.0023378783371299505\n",
      "#821 Loss: 0.002336920006200671\n",
      "#822 Loss: 0.0023359637707471848\n",
      "#823 Loss: 0.0023350082337856293\n",
      "#824 Loss: 0.0023340554907917976\n",
      "#825 Loss: 0.002333096694201231\n",
      "#826 Loss: 0.0023321439512073994\n",
      "#827 Loss: 0.0023311867844313383\n",
      "#828 Loss: 0.0023302286863327026\n",
      "#829 Loss: 0.0023292778059840202\n",
      "#830 Loss: 0.0023283222690224648\n",
      "#831 Loss: 0.0023273637052625418\n",
      "#832 Loss: 0.0023264100309461355\n",
      "#833 Loss: 0.0023254482075572014\n",
      "#834 Loss: 0.0023244949989020824\n",
      "#835 Loss: 0.002323535503819585\n",
      "#836 Loss: 0.0023225764743983746\n",
      "#837 Loss: 0.002321622334420681\n",
      "#838 Loss: 0.0023206640034914017\n",
      "#839 Loss: 0.002319705206900835\n",
      "#840 Loss: 0.0023187482729554176\n",
      "#841 Loss: 0.0023177897091954947\n",
      "#842 Loss: 0.0023168311454355717\n",
      "#843 Loss: 0.0023158686235547066\n",
      "#844 Loss: 0.002314911922439933\n",
      "#845 Loss: 0.0023139524273574352\n",
      "#846 Loss: 0.0023129964247345924\n",
      "#847 Loss: 0.002312036696821451\n",
      "#848 Loss: 0.002311078365892172\n",
      "#849 Loss: 0.0023101174738258123\n",
      "#850 Loss: 0.0023091596085578203\n",
      "#851 Loss: 0.0023081982508301735\n",
      "#852 Loss: 0.0023072347976267338\n",
      "#853 Loss: 0.0023062769323587418\n",
      "#854 Loss: 0.0023053153418004513\n",
      "#855 Loss: 0.0023043544497340918\n",
      "#856 Loss: 0.0023033986799418926\n",
      "#857 Loss: 0.0023024326656013727\n",
      "#858 Loss: 0.0023014741018414497\n",
      "#859 Loss: 0.0023005143739283085\n",
      "#860 Loss: 0.002299553481861949\n",
      "#861 Loss: 0.002298594219610095\n",
      "#862 Loss: 0.0022976300679147243\n",
      "#863 Loss: 0.002296670340001583\n",
      "#864 Loss: 0.002295706421136856\n",
      "#865 Loss: 0.002294748555868864\n",
      "#866 Loss: 0.002293783240020275\n",
      "#867 Loss: 0.0022928202524781227\n",
      "#868 Loss: 0.002291859593242407\n",
      "#869 Loss: 0.0022908993996679783\n",
      "#870 Loss: 0.002289933618158102\n",
      "#871 Loss: 0.0022889713291078806\n",
      "#872 Loss: 0.0022880099713802338\n",
      "#873 Loss: 0.0022870469838380814\n",
      "#874 Loss: 0.002286082599312067\n",
      "#875 Loss: 0.0022851191461086273\n",
      "#876 Loss: 0.0022841570898890495\n",
      "#877 Loss: 0.00228319363668561\n",
      "#878 Loss: 0.0022822280880063772\n",
      "#879 Loss: 0.00228126160800457\n",
      "#880 Loss: 0.002280300483107567\n",
      "#881 Loss: 0.002279336331412196\n",
      "#882 Loss: 0.0022783742751926184\n",
      "#883 Loss: 0.0022774089593440294\n",
      "#884 Loss: 0.0022764415480196476\n",
      "#885 Loss: 0.0022754797246307135\n",
      "#886 Loss: 0.0022745130117982626\n",
      "#887 Loss: 0.002273546764627099\n",
      "#888 Loss: 0.0022725812159478664\n",
      "#889 Loss: 0.002271618228405714\n",
      "#890 Loss: 0.0022706545423716307\n",
      "#891 Loss: 0.0022696880623698235\n",
      "#892 Loss: 0.0022687201853841543\n",
      "#893 Loss: 0.0022677555680274963\n",
      "#894 Loss: 0.002266788622364402\n",
      "#895 Loss: 0.0022658249363303185\n",
      "#896 Loss: 0.002264855895191431\n",
      "#897 Loss: 0.0022638929076492786\n",
      "#898 Loss: 0.002262924565002322\n",
      "#899 Loss: 0.0022619590163230896\n",
      "#900 Loss: 0.0022609897423535585\n",
      "#901 Loss: 0.002260024892166257\n",
      "#902 Loss: 0.0022590537555515766\n",
      "#903 Loss: 0.002258088905364275\n",
      "#904 Loss: 0.0022571226581931114\n",
      "#905 Loss: 0.0022561554796993732\n",
      "#906 Loss: 0.0022551873698830605\n",
      "#907 Loss: 0.0022542185615748167\n",
      "#908 Loss: 0.002253255806863308\n",
      "#909 Loss: 0.0022522821091115475\n",
      "#910 Loss: 0.0022513188887387514\n",
      "#911 Loss: 0.002250346355140209\n",
      "#912 Loss: 0.002249381970614195\n",
      "#913 Loss: 0.0022484115324914455\n",
      "#914 Loss: 0.0022474448196589947\n",
      "#915 Loss: 0.0022464708890765905\n",
      "#916 Loss: 0.002245502546429634\n",
      "#917 Loss: 0.002244536764919758\n",
      "#918 Loss: 0.0022435642313212156\n",
      "#919 Loss: 0.0022425984498113394\n",
      "#920 Loss: 0.002241630107164383\n",
      "#921 Loss: 0.0022406557109206915\n",
      "#922 Loss: 0.002239687368273735\n",
      "#923 Loss: 0.0022387171629816294\n",
      "#924 Loss: 0.0022377464920282364\n",
      "#925 Loss: 0.0022367790807038546\n",
      "#926 Loss: 0.0022358091082423925\n",
      "#927 Loss: 0.0022348423954099417\n",
      "#928 Loss: 0.002233867533504963\n",
      "#929 Loss: 0.00223289686255157\n",
      "#930 Loss: 0.0022319257259368896\n",
      "#931 Loss: 0.002230955520644784\n",
      "#932 Loss: 0.0022299832198768854\n",
      "#933 Loss: 0.002229014877229929\n",
      "#934 Loss: 0.002228041645139456\n",
      "#935 Loss: 0.0022270723711699247\n",
      "#936 Loss: 0.002226105658337474\n",
      "#937 Loss: 0.00222513428889215\n",
      "#938 Loss: 0.0022241591941565275\n",
      "#939 Loss: 0.00222318759188056\n",
      "#940 Loss: 0.002222212264314294\n",
      "#941 Loss: 0.002221246249973774\n",
      "#942 Loss: 0.0022202718537300825\n",
      "#943 Loss: 0.0022192979231476784\n",
      "#944 Loss: 0.002218327485024929\n",
      "#945 Loss: 0.0022173523902893066\n",
      "#946 Loss: 0.002216383581981063\n",
      "#947 Loss: 0.002215410117059946\n",
      "#948 Loss: 0.002214438747614622\n",
      "#949 Loss: 0.0022134636528789997\n",
      "#950 Loss: 0.0022124885581433773\n",
      "#951 Loss: 0.0022115185856819153\n",
      "#952 Loss: 0.0022105451207607985\n",
      "#953 Loss: 0.0022095697931945324\n",
      "#954 Loss: 0.0022086002863943577\n",
      "#955 Loss: 0.002207627519965172\n",
      "#956 Loss: 0.0022066505625844\n",
      "#957 Loss: 0.002205675933510065\n",
      "#958 Loss: 0.002204706659540534\n",
      "#959 Loss: 0.002203729934990406\n",
      "#960 Loss: 0.0022027562372386456\n",
      "#961 Loss: 0.002201780676841736\n",
      "#962 Loss: 0.0022008088417351246\n",
      "#963 Loss: 0.0021998314186930656\n",
      "#964 Loss: 0.002198859816417098\n",
      "#965 Loss: 0.0021978849545121193\n",
      "#966 Loss: 0.002196908462792635\n",
      "#967 Loss: 0.0021959375590085983\n",
      "#968 Loss: 0.00219495827332139\n",
      "#969 Loss: 0.002193986903876066\n",
      "#970 Loss: 0.002193011809140444\n",
      "#971 Loss: 0.0021920367144048214\n",
      "#972 Loss: 0.0021910606883466244\n",
      "#973 Loss: 0.0021900832653045654\n",
      "#974 Loss: 0.0021891086362302303\n",
      "#975 Loss: 0.002188134705647826\n",
      "#976 Loss: 0.00218715937808156\n",
      "#977 Loss: 0.002186183352023363\n",
      "#978 Loss: 0.002185206860303879\n",
      "#979 Loss: 0.0021842322312295437\n",
      "#980 Loss: 0.0021832589991390705\n",
      "#981 Loss: 0.002182276686653495\n",
      "#982 Loss: 0.0021812994964420795\n",
      "#983 Loss: 0.0021803276613354683\n",
      "#984 Loss: 0.0021793493069708347\n",
      "#985 Loss: 0.002178372349590063\n",
      "#986 Loss: 0.0021773958578705788\n",
      "#987 Loss: 0.002176423091441393\n",
      "#988 Loss: 0.0021754438057541847\n",
      "#989 Loss: 0.00217446475289762\n",
      "#990 Loss: 0.0021734880283474922\n",
      "#991 Loss: 0.00217251549474895\n",
      "#992 Loss: 0.0021715376060456038\n",
      "#993 Loss: 0.0021705608814954758\n",
      "#994 Loss: 0.002169583458453417\n",
      "#995 Loss: 0.002168601145967841\n",
      "#996 Loss: 0.0021676255855709314\n",
      "#997 Loss: 0.0021666502580046654\n",
      "#998 Loss: 0.0021656714379787445\n",
      "#999 Loss: 0.002164694247767329\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([0.5000, 1.0000], device='cuda:0')\n",
      "Output: \n",
      "tensor([0.9346], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NN = Neural_Network(device)\n",
    "# for i in range(1000):  # trains the NN 1,000 times\n",
    "#     print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "#     NN.train(X, y)\n",
    "for i in range(1000):  # trains the NN 1,000 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y_on_gpu - NN(X_on_gpu))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X_on_gpu, y_on_gpu)\n",
    "NN.saveWeights(NN)\n",
    "# NN.predict(xPredicted)\n",
    "NN.predict(xPredicted_on_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42709860fae64ed3042008c683475fc572b0a27a9cf85cc910105cf623802a05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
